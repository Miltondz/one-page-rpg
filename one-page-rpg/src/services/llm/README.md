# LLM Integration - SmolLM-360M-Instruct

Sistema de generaci√≥n de narrativa din√°mica usando un LLM local que se ejecuta completamente en el navegador.

## üéØ Caracter√≠sticas

- ‚úÖ **LLM Local**: Ejecuta SmolLM-360M-Instruct completamente en el navegador usando Transformers.js
- ‚úÖ **Cache Inteligente**: El modelo se descarga una vez y se cachea en el navegador
- ‚úÖ **Fallback Procedural**: Sistema de generaci√≥n procedural autom√°tico si el LLM falla
- ‚úÖ **Context-Aware**: Genera narrativa basada en el estado completo del juego
- ‚úÖ **M√∫ltiples Tipos**: Soporta escenas, di√°logos, combate, pensamientos, y m√°s
- ‚úÖ **TypeScript**: Completamente tipado con TypeScript

## üì¶ Instalaci√≥n

El paquete `@xenova/transformers` ya est√° instalado en el proyecto:

```bash
npm install @xenova/transformers
```

## üöÄ Uso B√°sico

### 1. Generar Descripci√≥n de Escena

```typescript
import { getLLMService } from './services/llm';

const llm = getLLMService();

const result = await llm.generateNarrative({
  context: {
    player: playerState,
    location: {
      id: 'griswald_plaza',
      name: 'Plaza de Griswald',
      description: 'Una plaza oscura y h√∫meda',
      type: 'city'
    },
    recentEvents: ['Derrotaste a un ladr√≥n', 'Encontraste una llave'],
    inventory: playerItems,
    worldState: gameState.world
  },
  type: 'scene_description',
  maxLength: 150,
  temperature: 0.7
});

console.log(result.text); // Narrativa generada
console.log(result.usedLLM); // true si us√≥ LLM, false si us√≥ fallback
```

### 2. Usar el NarrativeEngine Mejorado

```typescript
import { LLMNarrativeEngine } from './engine/LLMNarrativeEngine';

const engine = new LLMNarrativeEngine(scenesData, initialGameState);

// Pre-cargar el modelo al iniciar el juego
await engine.preloadLLM();

// Cargar escena con descripci√≥n enriquecida
const { scene, enhancedDescription } = await engine.loadSceneEnhanced(
  'scene_01',
  playerState,
  gameState,
  true // enhanceWithLLM
);

// Generar di√°logo NPC din√°mico
const dialogue = await engine.generateNPCDialogue(
  'Elara',
  'la misi√≥n del eco robado',
  playerState,
  gameState,
  currentScene
);

// Generar texto de combate
const combatText = await engine.generateCombatFlavor(
  'critical',
  playerState,
  gameState,
  'Guardia Corrupto'
);
```

## üé® Tipos de Narrativa Soportados

| Tipo | Descripci√≥n | Ejemplo de Uso |
|------|-------------|----------------|
| `scene_description` | Describe la escena actual | Entrada a una nueva ubicaci√≥n |
| `dialogue` | Di√°logo de NPCs | Conversaci√≥n con personajes |
| `combat_flavor` | Texto de sabor para combate | Golpes, cr√≠ticos, victorias |
| `item_discovery` | Descubrimiento de items | Encontrar objetos |
| `quest_update` | Actualizaci√≥n de misiones | Progreso en objetivos |
| `environmental_detail` | Detalles ambientales | Atm√≥sfera y clima |
| `character_thought` | Pensamientos del personaje | Introspecci√≥n |

## ‚öôÔ∏è Configuraci√≥n

### Configuraci√≥n por Defecto

```typescript
{
  modelId: 'HuggingFaceTB/SmolLM-360M-Instruct',
  enabled: true,
  useCache: true,
  defaultParams: {
    maxLength: 150,
    temperature: 0.7,
    topK: 50,
    topP: 0.9
  },
  timeout: 30000, // 30 segundos
  debug: false
}
```

### Personalizar Configuraci√≥n

```typescript
import { getLLMService } from './services/llm';

const llm = getLLMService({
  enabled: true,
  debug: true, // Ver logs detallados
  timeout: 60000, // 1 minuto
  defaultParams: {
    temperature: 0.8, // M√°s creativo
    maxLength: 200 // Textos m√°s largos
  }
});
```

### Deshabilitar LLM (Solo Fallback)

```typescript
const llm = getLLMService({ enabled: false });
// O durante runtime:
llm.setEnabled(false);
```

## üîß API del LLMService

### `generateNarrative(params)`

Genera narrativa usando el LLM o fallback.

```typescript
interface NarrativeGenerationParams {
  context: LLMContext;
  type: NarrativeType;
  specificPrompt?: string;
  maxLength?: number;
  temperature?: number;
}

const result = await llm.generateNarrative({
  context,
  type: 'dialogue',
  specificPrompt: 'Elara habla sobre el misterio',
  maxLength: 100,
  temperature: 0.75
});
```

### `preload()`

Pre-carga el modelo LLM. √ötil para inicializaci√≥n.

```typescript
await llm.preload();
```

### `getStatus()`

Obtiene el estado actual del servicio.

```typescript
const status = llm.getStatus();
// {
//   enabled: true,
//   modelLoaded: true,
//   isLoading: false,
//   hasError: false,
//   error?: string
// }
```

### `setEnabled(enabled)`

Habilita/deshabilita el LLM en runtime.

```typescript
llm.setEnabled(false); // Solo usar fallback
llm.setEnabled(true);  // Volver a usar LLM
```

### `unload()`

Libera memoria del modelo.

```typescript
llm.unload();
```

## üìù Contexto del Juego (LLMContext)

El contexto del juego que se pasa al LLM incluye:

```typescript
interface LLMContext {
  player: Player;              // Estado del jugador
  location: {                  // Ubicaci√≥n actual
    id: string;
    name: string;
    description: string;
    type: 'city' | 'dungeon' | 'wilderness' | 'interior';
  };
  recentEvents: string[];      // Eventos recientes (√∫ltimos 5-10)
  inventory: Item[];           // Items del jugador
  worldState: WorldState;      // Estado global del mundo
  additionalContext?: {        // Contexto adicional opcional
    presentNPCs?: string[];
    presentEnemies?: string[];
    timeOfDay?: string;
    weather?: string;
    activeQuest?: string;
  };
}
```

## üé≠ Prompts Din√°micos

El sistema usa templates din√°micos que inyectan autom√°ticamente:

- Estado del jugador (salud, nivel, atributos, inventario)
- Ubicaci√≥n y ambiente (tipo, clima, hora del d√≠a)
- Eventos recientes (√∫ltimas acciones del jugador)
- NPCs y enemigos presentes
- Misi√≥n activa

**Ejemplo de prompt generado internamente:**

```
Eres un narrador maestro para un juego de rol dark fantasy...

CONTEXTO DEL JUEGO:
Kael, nivel 2 human adventurer | Salud: 75% (6/8) | STR:3 AGI:2 INT:1 | Items: 4 | Oro: 25
Ubicaci√≥n: Plaza de Griswald (city) | Hora: night | Clima: fog

Eventos recientes:
1. Derrotaste a un Guardia Corrupto
2. Encontraste una Llave Antigua
3. Hablaste con Elara

TAREA: Describe la escena actual desde la perspectiva del jugador...
```

## üîÑ Sistema de Fallback

Si el LLM falla o est√° deshabilitado, el sistema usa generaci√≥n procedural:

```typescript
// Autom√°ticamente usa fallback si:
// - LLM est√° deshabilitado
// - Error al cargar el modelo
// - Timeout en la generaci√≥n
// - Error en el contexto

const result = await llm.generateNarrative(params);
console.log(result.usedLLM); // false si us√≥ fallback
```

El fallback genera texto coherente y contextual usando templates predefinidos y el estado del juego.

## üß™ Testing

```typescript
import { getLLMService, resetLLMService } from './services/llm';

// Test con LLM deshabilitado
const llm = getLLMService({ enabled: false });
const result = await llm.generateNarrative(params);
expect(result.usedLLM).toBe(false);

// Limpiar para siguiente test
resetLLMService();
```

## üìä Performance

- **Primera carga**: ~3-5 segundos (descarga modelo)
- **Cargas posteriores**: Instant√°neo (cache del navegador)
- **Generaci√≥n**: ~500-2000ms dependiendo del hardware
- **Tama√±o del modelo**: ~140MB (se cachea localmente)

## üêõ Debug

Habilitar modo debug para ver logs detallados:

```typescript
const llm = getLLMService({ debug: true });

// Ver√°s logs como:
// [LLMService] Initialized with config: {...}
// [LLMService] Loading model: HuggingFaceTB/SmolLM-360M-Instruct
// [LLMService] Loading: model.onnx - 45%
// [LLMService] Model loaded successfully in 3245ms
// [LLMService] Generating with prompt: ...
// [LLMService] Generated in 1234ms: ...
```

## üö® Manejo de Errores

El sistema maneja autom√°ticamente:

- Error al cargar modelo ‚Üí usa fallback
- Timeout en generaci√≥n ‚Üí usa fallback
- Contexto inv√°lido ‚Üí usa fallback
- Modelo no soportado ‚Üí usa fallback

Todos los errores se registran en consola pero no rompen el juego.

## üìö Recursos

- [Transformers.js](https://huggingface.co/docs/transformers.js)
- [SmolLM-360M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct)
- [ONNX Runtime](https://onnxruntime.ai/)

## üéÆ Integraci√≥n con el Juego

El sistema LLM est√° completamente integrado con:

- ‚úÖ `NarrativeEngine` - Motor narrativo base
- ‚úÖ `LLMNarrativeEngine` - Motor mejorado con LLM
- ‚úÖ `CombatEngine` - Sistema de combate
- ‚úÖ `GameState` - Estado global del juego
- ‚úÖ UI Components - Componentes React

## üìã Pr√≥ximas Mejoras

- [ ] Sistema de tracking de eventos para mejor contexto
- [ ] Cache de generaciones comunes
- [ ] Ajuste fino del modelo con datos del juego
- [ ] Soporte para m√∫ltiples idiomas
- [ ] Generaci√≥n de misiones procedurales
- [ ] NPCs con memoria de conversaciones

---

**Implementado**: ‚úÖ Epic completa - Local LLM Integration (SmolLM-360M-Instruct)
